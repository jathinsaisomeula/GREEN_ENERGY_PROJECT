# Import necessary modules from Airflow and Python
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import os # For path manipulation

# Define default arguments for the DAG
default_args = {
    'owner': 'jathinsai', # The owner of the DAG
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=3),
}

# Define a Python function to simulate processing the generated data
# In a real scenario, this would read the output of generate_green_energy_data.py
# and perform operations like validation, aggregation, or saving to a database.
def process_generated_data_stub(**kwargs):
    """
    A stub function to simulate processing the generated green energy data.
    In a real project, this would involve reading the output file or stream
    from the data generation script.
    """
    print("Simulating processing of generated green energy data...")
    # For now, let's just confirm the generation script path for debugging
    home_dir = os.path.expanduser("~")
    generator_script_path = os.path.join(home_dir, "generate_green_energy_data.py")
    print(f"Assuming data was generated by: {generator_script_path}")
    print("This task would now read, transform, and store that data.")
    # Example: In a real scenario, if generate_green_energy_data.py
    # creates a CSV, you might read it here:
    # try:
    #     with open('/path/to/generated_data.csv', 'r') as f:
    #         data = f.read()
    #     print(f"Successfully read generated data (first 100 chars): {data[:100]}...")
    # except FileNotFoundError:
    #     print("Generated data file not found. Ensure the generation script creates it.")
    print("Processing stub complete.")

# Instantiate the DAG for our Green Energy Project
with DAG(
    dag_id='green_energy_data_pipeline', # Unique ID for our Green Energy DAG
    default_args=default_args,
    description='Pipeline to generate and process green energy data',
    schedule_interval=timedelta(hours=6), # Example: Run every 6 hours
    start_date=datetime(2025, 7, 12), # Set to today's date
    catchup=False,
    tags=['green_energy', 'data_generation', 'project'],
) as dag:
    # Task 1: Start message for the pipeline
    start_pipeline_msg = BashOperator(
        task_id='start_green_energy_pipeline',
        bash_command='echo "Starting Green Energy Data Pipeline..."',
    )

    # Task 2: Execute the existing green energy data generation script
    # Make sure 'generate_green_energy_data.py' is in your home directory or provide its full path
    generate_data = BashOperator(
        task_id='generate_green_energy_data',
        bash_command='python3 ~/generate_green_energy_data.py', # Assumes python3 and script in home dir
    )

    # Task 3: Simulate processing the generated data
    process_data = PythonOperator(
        task_id='process_generated_green_data',
        python_callable=process_generated_data_stub,
        provide_context=True,
    )

    # Task 4: End message
    end_pipeline_msg = BashOperator(
        task_id='end_green_energy_pipeline',
        bash_command='echo "Green Energy Data Pipeline finished!"',
    )

    # Define the task dependencies for our Green Energy workflow
    # Flow: Start -> Generate Data -> Process Data -> End
    start_pipeline_msg >> generate_data >> process_data >> end_pipeline_msg